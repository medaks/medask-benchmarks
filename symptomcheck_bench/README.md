# SymptomCheck Bench

An OSCE-style benchmark designed to evaluate the diagnostic accuracy of Large Language Model (LLM) based medical agents in symptom assessment conversations.

## Overview

SymptomCheck Bench simulates medical consultations through a structured four-step process:
1. **Initialization**: Selection of a clinical vignette
2. **Dialogue**: Simulated conversation between an LLM agent and patient
3. **Diagnosis**: Generation of top 5 differential diagnoses
4. **Evaluation**: Automated assessment of diagnostic accuracy

For more details about the benchmark, methodology, and results, read our blog post:
[Introducing SymptomCheck Bench](https://medask.tech/blogs/introducing-symptomcheck-bench/)

## Installation

From the project root directory:

```bash
# Create and activate environment
conda create -n medask-benchmarks python=3.12
conda activate medask-benchmarks

# Install dependencies
pip install -r requirements/development.txt
pip install -e .

# Set API key
export KEY_OPENAI="sk-..." # Set your API key in an ENV variable.
```

## Usage

### Inspecting Available Options

```bash
cd symptomcheck_bench

# Run without any arguments to see all available options
python main.py

# Output
usage: main.py [-h] [--doctor_llm DOCTOR_LLM] [--patient_llm PATIENT_LLM]
               [--evaluator_llm EVALUATOR_LLM] --file {avey,agentclinic}
               [--num_vignettes NUM_VIGNETTES] [--num_experiments NUM_EXPERIMENTS]
               [--comment COMMENT] [--result_name_suffix RESULT_NAME_SUFFIX]
main.py: error: the following arguments are required: --file
```

### Basic Usage

```bash
# Run benchmark on 3 random vignettes from Avey dataset, repeat 2 times
# Use gpt-4o for the LLM simulating the doctor
# By default, gpt-4o-mini is used for the LLM simulating the patient
# By default, gpt-4o is used to evaluate the correctness of the resulting diagnoses

python main.py --file=avey --doctor_llm=gpt-4o --num_vignettes=3 --num_experiments=2
```

### Example Output

```
2024-10-28 21:52:59,084 - [INFO] - benchmark - Running experiment over vignettes [68, 166, 334]
2024-10-28 21:53:24,619 - [INFO] - benchmark - Dumping results to results/2024-10-28T21:52:59_gpt-4o_3.json

position=1      Nephrolithiasis : [Kidney stones, Ureteral obstruction, Urinary tract infection, Pyelonephritis, Renal colic]
position=1      Measles : [Measles, Viral exanthem, Roseola, Rubella, Scarlet fever]
position=1      Cerebral Stroke : [Ischemic stroke, Hemorrhagic stroke, Transient ischemic attack (TIA), Bell's palsy, Intracranial hemorrhage]

2024-10-28 21:53:27,234 - [INFO] - benchmark.evaluate - Results of run i=0
   positions=[1.0, 1.0, 1.0]
   Number of correct diagnoses: 3 / 3
   Average position of correct diagnosis: 1.0

position=1      Nephrolithiasis : [Kidney stones, Ureteral stones, Renal colic, Hematuria, Urinary tract obstruction]
position=1      Measles : [Measles, Viral exanthem, Rubella, Scarlet fever, Roseola]
position=1      Cerebral Stroke : [Acute Ischemic Stroke, Transient Ischemic Attack, Hemorrhagic Stroke, Bell's Palsy, Migraine with Aura]

2024-10-28 21:53:30,660 - [INFO] - benchmark.evaluate - Results of run i=1
   positions=[1.0, 1.0, 1.0]
   Number of correct diagnoses: 3 / 3
   Average position of correct diagnosis: 1.0
```

## Understanding the Output

### Vignette Selection
```
2024-10-28 21:52:59,084 - [INFO] - benchmark - Running experiment over vignettes [68, 166, 334]
```
Shows the indices of the vignettes used in the experiment.

### Results Storage
```
2024-10-28 21:53:24,619 - [INFO] - benchmark - Dumping results to results/2024-10-28T21:52:59_gpt-4o_3.json
```
All experiment data is saved to disk as a timestamped JSON file.

### Individual Diagnosis Evaluation
```
position=1      Nephrolithiasis : [Kidney stones, Ureteral obstruction, Urinary tract infection, Pyelonephritis, Renal colic]
```
This shows:
- **Nephrolithiasis**: The correct diagnosis (kidney stones)
- **[Kidney stones, ...]**: The 5 most likely diagnoses generated by the symptom checker
- **position=1**: The evaluator LLM determined the correct diagnosis was found in position 1

### Aggregated Results
```
2024-10-28 21:53:27,234 - [INFO] - benchmark.evaluate - Results of run i=0
   positions=[1.0, 1.0, 1.0]
   Number of correct diagnoses: 3 / 3
   Average position of correct diagnosis: 1.0
```
Shows:
- **positions=[1.0, 1.0, 1.0]**: Position of correct diagnosis for each vignette
- **3 / 3**: Number of correct diagnoses out of total
- **Average position**: Mean position of correct diagnoses (1.0 = all correct diagnoses were ranked first)

## Command Line Arguments

- `--doctor_llm`: LLM model for the doctor agent (default: varies by model)
- `--patient_llm`: LLM model for the patient simulator (default: gpt-4o-mini)
- `--evaluator_llm`: LLM model for evaluating diagnoses (default: gpt-4o)
- `--file`: Vignette dataset to use (`avey` or `agentclinic`)
- `--num_vignettes`: Number of vignettes to evaluate (default: all)
- `--num_experiments`: Number of experimental runs (default: 1)
- `--comment`: Optional comment for the experiment
- `--result_name_suffix`: Suffix for result filename

## Available Datasets

- **avey**: Clinical vignettes from the Avey dataset
- **agentclinic**: Clinical vignettes from the AgentClinic dataset

## Output Files

Results are saved as JSON files in the `results/` directory with the format:
```
results/YYYY-MM-DDTHH:MM:SS_model_numvignettes.json
```

## Analyzing Results

After running experiments, you can programmatically analyze the results by loading the JSON files. The results contain detailed information about:
- Conversation transcripts
- Generated diagnoses
- Evaluation scores
- Timing information
- Model configurations

See the example screenshot in the main repository README for an illustration of how to inspect results programmatically.

## Research Applications

This benchmark is useful for:
- Evaluating diagnostic accuracy of medical AI systems
- Comparing different LLM architectures for medical applications
- Testing the impact of different conversation strategies
- Validating medical AI before clinical deployment
- Academic research on AI in healthcare

## Citation

If you use this benchmark in your research, please cite the associated publication and reference our blog post: [Introducing SymptomCheck Bench](https://medask.tech/blogs/introducing-symptomcheck-bench/)
